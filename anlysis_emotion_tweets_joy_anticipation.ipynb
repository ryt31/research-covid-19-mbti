{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter BERT と xlm-roberta-base でコロナ禍後のツイートにおいて Joy と Anticipation の割合が増加した。\n",
    "#### コロナ禍前の Joy, Anticipation のツイートとコロナ禍後の Joy, Anticipation のツイートを抜き出し、そこから特徴語を抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from denoicer import Denoicer\n",
    "from mecab_wakati import MecabWakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoicer = Denoicer()\n",
    "mecab_wakati = MecabWakati()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_with_before_after(isReply=False):\n",
    "    path = './database/tweets_with_before_or_after.tsv'\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    if isReply:\n",
    "        df = df[df['tweet_text'].str.match('@(\\w+) ')]\n",
    "    else:\n",
    "        df = df[df['tweet_text'].str.match('(?!@(\\w+) )')]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = read_tweets_with_before_after(isReply=False)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion を指定してコロナ禍前後で分けて辞書方でツイートを返す\n",
    "def filter_tweets_by_emotion(emotion: str, df: pd.DataFrame, df_emo: pd.DataFrame) -> dict:\n",
    "    results = {\n",
    "        'before': [],\n",
    "        'after': []\n",
    "    }\n",
    "    for tw_id, emo in zip(df_emo['tweet_id'], df_emo['emo']):\n",
    "        if type(emo) != float:\n",
    "            emos = emo.strip().split('|')\n",
    "            if emotion in emos:\n",
    "                d = df[df['tweet_id'] == tw_id]\n",
    "                before_or_after = d['before_or_after'].values[0]\n",
    "                tweet = d['tweet_text'].values[0]\n",
    "                results[before_or_after].append(tweet)\n",
    "    return results\n",
    "\n",
    "# 取得したツイートを出力\n",
    "def output_tweets(path, save_folder_path, tweets: list):\n",
    "    if not os.path.exists(save_folder_path):\n",
    "        os.makedirs(save_folder_path)\n",
    "    with open(path, 'w', encoding='utf8') as f:\n",
    "        for tweet in tweets:\n",
    "            f.write(f'{tweet}\\n')\n",
    "\n",
    "\n",
    "'''\n",
    "tsv_paths = ['./database/emotion/estimate_result/tweet/xlm-roberta-base.tsv',\n",
    "         './database/emotion/estimate_result/tweet/Twitter-twhin-bert-base.tsv']\n",
    "before_or_after = ['before', 'after']\n",
    "emos = ['Joy', 'Anticipation']\n",
    "\n",
    "for tsv_path in tsv_paths:\n",
    "    df_emo = pd.read_csv(tsv_path, sep='\\t')\n",
    "    for emo in emos:\n",
    "        dic = filter_tweets_by_emotion(emo, df=df, df_emo=df_emo)\n",
    "        for ba in before_or_after:\n",
    "            tweets = dic[ba]\n",
    "            model_name = tsv_path.split('/')[-1][:-4]\n",
    "            save_folder_path = f'./database/emotion/each_emotion_tweets/{model_name}/{ba}'\n",
    "            path = f'{save_folder_path}/{emo}_tweets.txt'\n",
    "            output_tweets(path=path,save_folder_path=save_folder_path,tweets=tweets)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words ベクトルを付与し頻出単語をアウトプット\n",
    "def create_bag_of_words_vector_and_output(path, output_path):\n",
    "    docs = []\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        tweets = f.readlines()\n",
    "        for tweet in tweets:\n",
    "            tweet = denoicer.normalize_text(tweet.strip())\n",
    "            tweet = mecab_wakati.wakati_sentence(tweet)\n",
    "            docs.append(' '.join(tweet))\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    values = X.toarray()\n",
    "    feature_names = {}\n",
    "    for k, v in vectorizer.vocabulary_.items():\n",
    "        feature_names[v] = k\n",
    "        \n",
    "    outputs = []\n",
    "    dic = {}\n",
    "    for value in tqdm(values):\n",
    "        for i, v in enumerate(value):\n",
    "            if v > 0:\n",
    "                word = feature_names[i]\n",
    "                if word not in dic:\n",
    "                    dic[word] = v\n",
    "                else:\n",
    "                    dic[word] += v\n",
    "    top20 = []\n",
    "    for k, v in sorted(dic.items(), key=lambda x: x[1], reverse=True):\n",
    "        if len(top20) >= 20:\n",
    "            break\n",
    "        if k not in denoicer.stop_words:\n",
    "            top20.append([k, v])\n",
    "\n",
    "    for top in top20:\n",
    "        outputs.append(f'{top[0]}: {top[1]}\\n')\n",
    "\n",
    "    if not os.path.exists('./covid-19_feature_words/emotion/bag-of-words'):\n",
    "        os.makedirs('./covid-19_feature_words/emotion/bag-of-words')\n",
    "    with open(output_path, 'w', encoding='utf8') as f:\n",
    "        for o in outputs:\n",
    "            f.write(o)\n",
    "\n",
    "'''\n",
    "paths = ['./database/emotion/each_emotion_tweets/Twitter-twhin-bert-base',\n",
    "         './database/emotion/each_emotion_tweets/xlm-roberta-base']\n",
    "before_or_after = ['before', 'after']\n",
    "emos = ['Joy', 'Anticipation']\n",
    "for path in paths:\n",
    "    for ba in before_or_after:\n",
    "        for emo in emos:\n",
    "            p = f'{path}/{ba}/{emo}_tweets.txt'\n",
    "            model_name = path.split('/')[-1]\n",
    "            output_path = f'./covid-19_feature_words/emotion/bag-of-words/{emo}_{ba}_{model_name}.txt'\n",
    "            create_bag_of_words_vector_and_output(path=p, output_path=output_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words の特徴語から WordCloud を作成\n",
    "def create_wordcloud_bag_of_words(path, save_path_folder, emo, before_or_after, model_name):\n",
    "    word_cloud = WordCloud(width=480, height=320, background_color=\"white\", font_path='/usr/share/fonts/truetype/takao-gothic/TakaoGothic.ttf')\n",
    "    dic = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        word_tmp = []\n",
    "        values_tmp = []\n",
    "        for l in lines:\n",
    "            split = l.strip().replace('\\t', '').replace(' ', '').split(':')\n",
    "            word_tmp.append(split[0])\n",
    "            values_tmp.append(float(split[1]))\n",
    "    for w, v in zip(word_tmp, values_tmp):\n",
    "        dic[w] = v\n",
    "    if not os.path.exists(save_path_folder):\n",
    "        os.makedirs(save_path_folder)\n",
    "    result = word_cloud.generate_from_frequencies(dic)\n",
    "    with open(f'{save_path_folder}/{emo}_{before_or_after}_{model_name}.svg', 'w', encoding='utf-8') as svg:\n",
    "        svg.write(result.to_svg())\n",
    "\n",
    "'''\n",
    "models = ['Twitter-twhin-bert-base', 'xlm-roberta-base']\n",
    "before_or_after = ['before', 'after']\n",
    "emos = ['Joy', 'Anticipation']\n",
    "\n",
    "for model in models:\n",
    "    for ba in before_or_after:\n",
    "        for emo in emos:\n",
    "            path = f'covid-19_feature_words/emotion/bag-of-words/{emo}_{ba}_{model}.txt'\n",
    "            create_wordcloud_bag_of_words(path=path, save_path_folder='covid-19_feature_words/emotion/bag-of-words/word_cloud', emo=emo, before_or_after=ba, model_name=model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF ベクトルを付与し各MBTIタイプの特徴語をアウトプット\n",
    "def create_tf_idf_vector_and_output(base_path, emo, model, save_folder_path, max_df=0.9):\n",
    "    docs = []\n",
    "    before_or_after = ['before', 'after']\n",
    "    for ba in before_or_after:\n",
    "        path = f'{base_path}/{ba}/{emo}_tweets.txt'\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            tweets = f.readlines()\n",
    "            for tweet in tqdm(tweets):\n",
    "                tweet = denoicer.normalize_text(tweet.strip())\n",
    "                tweet = mecab_wakati.wakati_sentence(tweet)\n",
    "            docs.append(' '.join(tweet))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df)  # 文書全体の90%以上で出現する単語は無視する\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    values = X.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    for ba, vec in zip(before_or_after, values):\n",
    "        outputs = []\n",
    "        for w_id, tfidf in sorted(enumerate(vec), key=lambda x: x[1], reverse=True)[:20]:\n",
    "            word = feature_names[w_id]\n",
    "            outputs.append('{0:s}: {1:f}\\n'.format(word, tfidf))\n",
    "        if not os.path.exists(save_folder_path):\n",
    "            os.makedirs(save_folder_path)\n",
    "        with open(f'{save_folder_path}/{emo}_{ba}_{model}_feature_words_top_20.txt', 'w', encoding='utf8') as f:\n",
    "            for o in outputs:\n",
    "                f.write(o)\n",
    "\n",
    "'''\n",
    "models = ['Twitter-twhin-bert-base', 'xlm-roberta-base']\n",
    "emos = ['Joy', 'Anticipation']\n",
    "\n",
    "for model in models:\n",
    "    for emo in emos:\n",
    "        base_path = f'./database/emotion/each_emotion_tweets/{model}'\n",
    "        save_folder_path = './covid-19_feature_words/emotion/tf-idf'\n",
    "        create_tf_idf_vector_and_output(base_path=base_path, emo=emo, model=model, save_folder_path=save_folder_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF の特徴語から WordCloud を作成\n",
    "def create_wordcloud_tf_idf(path, save_path_folder, emo, before_or_after, model_name):\n",
    "    word_cloud = WordCloud(width=480, height=320, background_color=\"white\", font_path='/usr/share/fonts/truetype/takao-gothic/TakaoGothic.ttf')\n",
    "    dic = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        word_tmp = []\n",
    "        values_tmp = []\n",
    "        for l in lines:\n",
    "            split = l.strip().replace('\\t', '').replace(' ', '').split(':')\n",
    "            if float(split[1]) != 0:\n",
    "                word_tmp.append(split[0])\n",
    "                values_tmp.append(float(split[1]))\n",
    "        for w, v in zip(word_tmp, values_tmp):\n",
    "            for _ in range(int(1*float(v)/float(values_tmp[len(word_tmp)-1]))):\n",
    "                if w not in dic:\n",
    "                    dic[w] = 1\n",
    "                else:\n",
    "                    dic[w] += 1\n",
    "    if not os.path.exists(save_path_folder):\n",
    "        os.makedirs(save_path_folder)\n",
    "    result = word_cloud.generate_from_frequencies(dic)\n",
    "    with open(f'{save_path_folder}/{emo}_{before_or_after}_{model_name}.svg', 'w', encoding='utf-8') as svg:\n",
    "        svg.write(result.to_svg())\n",
    "\n",
    "\n",
    "models = ['Twitter-twhin-bert-base', 'xlm-roberta-base']\n",
    "before_or_after = ['before', 'after']\n",
    "emos = ['Joy', 'Anticipation']\n",
    "\n",
    "for model in models:\n",
    "    for ba in before_or_after:\n",
    "        for emo in emos:\n",
    "            path = f'covid-19_feature_words/emotion/tf-idf/{emo}_{ba}_{model}_feature_words_top_20.txt'\n",
    "            create_wordcloud_tf_idf(path=path, save_path_folder='covid-19_feature_words/emotion/tf-idf/word_cloud', emo=emo, before_or_after=ba, model_name=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "210b98d706396c77d792f59245ce4968b85b3888226289c4d26021256b3fa3b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
