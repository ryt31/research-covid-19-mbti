{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from denoicer import Denoicer\n",
    "from mbti_util import MbtiUtil\n",
    "from mecab_wakati import MecabWakati\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MecabWakati()\n",
    "mbti_util = MbtiUtil()\n",
    "denoice = Denoicer()\n",
    "word_cloud = WordCloud(width=480, height=320, background_color=\"white\", font_path='/usr/share/fonts/truetype/takao-gothic/TakaoGothic.ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_with_before_after(isReply=False):\n",
    "    path = './database/tweets_with_before_or_after.tsv'\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    if isReply:\n",
    "        df = df[df['tweet_text'].str.match('@(\\w+) ')]\n",
    "    else:\n",
    "        df = df[df['tweet_text'].str.match('(?!@(\\w+) )')]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = read_tweets_with_before_after(isReply=False)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"コロナ|武漢|Covid|COVID|ワクチン|パンデミック|マスク|自粛|クラスター|蔓延防止|マンボウ|まん延防止|給付金\"\n",
    "df_covid = df[df['tweet_text'].str.contains(f'{query}')]\n",
    "df_covid = df_covid[df_covid['before_or_after'] == 'after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words ベクトルを付与し各MBTIタイプの頻出単語をアウトプット\n",
    "def create_bag_of_words_vector_and_output(path):\n",
    "    docs = []\n",
    "    for m_type in mbti_util.m_types:\n",
    "        tweets = df_covid[df_covid['m_type_en'] == m_type]['tweet_text'].values\n",
    "        words = []\n",
    "        for tweet in tweets:\n",
    "            text = denoice.normalize_text(tweet)\n",
    "            words += mecab.wakati_sentence(text)\n",
    "        docs.append(' '.join(words).strip())\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    values = X.toarray()\n",
    "    feature_names = {}\n",
    "    for k, v in vectorizer.vocabulary_.items():\n",
    "        feature_names[v] = k\n",
    "        \n",
    "    outputs = []\n",
    "    for m_type, value in zip(mbti_util.m_types, values):\n",
    "        dic = {}\n",
    "        for i, v in enumerate(value):\n",
    "            dic[feature_names[i]] = v\n",
    "        top20 = []\n",
    "        for k, v in sorted(dic.items(), key=lambda x: x[1], reverse=True):\n",
    "            if len(top20) >= 20:\n",
    "                break\n",
    "            if k not in denoice.stop_words:\n",
    "                top20.append([k, v])\n",
    "        outputs.append(f'{m_type}\\n')\n",
    "        for top in top20:\n",
    "            outputs.append(f'\\t{top[0]}: {top[1]}\\n')\n",
    "    outputs.append('EOF')\n",
    "\n",
    "    if not os.path.exists('./covid-19_feature_words/bag-of-words'):\n",
    "        os.makedirs('./covid-19_feature_words/bag-of-words')\n",
    "    with open(path, 'w', encoding='utf8') as f:\n",
    "        for o in outputs:\n",
    "            f.write(o)\n",
    "            \n",
    "# path = 'covid-19_feature_words/bag-of-words/feature_words_top_20.txt'\n",
    "# create_bag_of_words_vector_and_output(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words の特徴語から WordCloud を作成\n",
    "def create_wordcloud_bag_of_words(path, save_path_folder):\n",
    "    dic = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        word_tmp = []\n",
    "        values_tmp = []\n",
    "        for l in lines:\n",
    "            if '\\t' not in l:\n",
    "                if word_tmp and values_tmp:\n",
    "                    for w, v in zip(word_tmp, values_tmp):\n",
    "                        dic[m_type][w] = v\n",
    "                m_type = l.strip()\n",
    "                dic[m_type] = {}\n",
    "                word_tmp = []\n",
    "                values_tmp = []\n",
    "            else:\n",
    "                split = l.strip().replace('\\t', '').replace(' ', '').split(':')\n",
    "                word_tmp.append(split[0])\n",
    "                values_tmp.append(float(split[1]))\n",
    "    if not os.path.exists(save_path_folder):\n",
    "        os.makedirs(save_path_folder)\n",
    "    for m_type, d in dic.items():\n",
    "        if d != {}:\n",
    "            result = word_cloud.generate_from_frequencies(d)\n",
    "            with open(f'{save_path_folder}/{m_type}.svg', 'w', encoding='utf-8') as svg:\n",
    "                svg.write(result.to_svg())\n",
    "            print(f'{m_type} completed.')\n",
    "\n",
    "# path = 'covid-19_feature_words/bag-of-words/feature_words_top_20.txt'\n",
    "# create_wordcloud_bag_of_words(path=path, save_path_folder='covid-19_feature_words/bag-of-words/word_cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF ベクトルを付与し各MBTIタイプの特徴語をアウトプット\n",
    "def create_tf_idf_vector_and_output(path, max_df=0.9):\n",
    "    docs = []\n",
    "    for m_type in mbti_util.m_types:\n",
    "        tweets = df_covid[df_covid['m_type_en'] == m_type]['tweet_text'].values\n",
    "        words = []\n",
    "        for tweet in tweets:\n",
    "            text = denoice.normalize_text(tweet)\n",
    "            words += mecab.wakati_sentence(text)\n",
    "        docs.append(' '.join(words).strip())\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df)  # 文書全体の90%以上で出現する単語は無視する\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    values = X.toarray()\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    outputs = []\n",
    "    for doc_no, vec in zip(range(len(docs)), values):\n",
    "        title = mbti_util.m_types[doc_no]\n",
    "        outputs.append(f'{title}\\n')\n",
    "        for w_id, tfidf in sorted(enumerate(vec), key=lambda x: x[1], reverse=True)[:20]:\n",
    "            word = feature_names[w_id]\n",
    "            outputs.append('\\t{0:s}: {1:f}\\n'.format(word, tfidf))\n",
    "    if not os.path.exists('covid-19_feature_words/tf-idf'):\n",
    "        os.makedirs('covid-19_feature_words/tf-idf')\n",
    "    with open(path, 'w', encoding='utf8') as f:\n",
    "        for o in outputs:\n",
    "            f.write(o)\n",
    "\n",
    "# path = 'covid-19_feature_words/tf-idf/feature_words_top_20.txt'\n",
    "# create_tf_idf_vector_and_output(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF の特徴語から WordCloud を作成\n",
    "def create_wordcloud_tf_idf(path, save_path_folder):\n",
    "    dic = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "        word_tmp = []\n",
    "        values_tmp = []\n",
    "        for l in lines:\n",
    "            if '\\t' not in l:\n",
    "                if word_tmp and values_tmp:\n",
    "                    for w, v in zip(word_tmp, values_tmp):\n",
    "                        for _ in range(int(1*float(v)/float(values_tmp[len(word_tmp)-1]))):\n",
    "                            if w not in dic[m_type]:\n",
    "                                dic[m_type][w] = 1\n",
    "                            else:\n",
    "                                dic[m_type][w] += 1\n",
    "                m_type = l.strip()\n",
    "                dic[m_type] = {}\n",
    "                word_tmp = []\n",
    "                values_tmp = []\n",
    "            else:\n",
    "                split = l.strip().replace('\\t', '').replace(' ', '').split(':')\n",
    "                word_tmp.append(split[0])\n",
    "                values_tmp.append(float(split[1]))\n",
    "    if not os.path.exists(save_path_folder):\n",
    "        os.makedirs(save_path_folder)\n",
    "    for m_type, d in dic.items():\n",
    "        if d != {}:\n",
    "            result = word_cloud.generate_from_frequencies(d)\n",
    "            with open(f'{save_path_folder}/{m_type}.svg', 'w', encoding='utf-8') as svg:\n",
    "                svg.write(result.to_svg())\n",
    "            print(f'{m_type} completed.')\n",
    "\n",
    "path = 'covid-19_feature_words/tf-idf/feature_words_top_20.txt'\n",
    "create_wordcloud_tf_idf(path=path, save_path_folder='covid-19_feature_words/tf-idf/word_cloud')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "210b98d706396c77d792f59245ce4968b85b3888226289c4d26021256b3fa3b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
